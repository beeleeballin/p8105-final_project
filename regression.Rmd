---
title: "Regression Analyses"
output: 
  html_document:
    toc: true
    toc_float: true
    code_folding: hide
---

# Part 1: State Level Trends, 2001-2015

## Data

We begin by reading in the combined health outcomes data, creating seasonal variables, and the combined air pollution and UV radiation data to produce an analysis at the state level. We quickly noticed that some of our variables in the air pollution and UV data were strongly correlated with each other, so we omitted several for the purposes of our regression analysis (collinearity violates our assumptions); the final correlation matrix is given below.

```{r, warning=FALSE, message=FALSE}
library(tidyverse)

outcomes_state <- read_csv("data/lc_mel_asthma_state.csv")
outcomes_county <- read_csv("data/lc_mel_asthma_county.csv")
ap_uv <- read_csv("ap/ap_uv/apuv.csv")

#State-level analysis - fewer predictors
ap_uv_state_slim <- ap_uv %>%
  select(-c(countyfips, county,
            pm25_max_pred, pm25_mean_pred, pm25_pop_pred,
            o3_max_pred, o3_mean_pred, o3_pop_pred,
            i310, i305, i380, edr)) %>%
  group_by(state, year, season) %>%
  #Take medians across county for each season
  summarize(across(pm25_med_pred:edd, median)) %>%
  pivot_wider(names_from = season, values_from = pm25_med_pred:edd)

#State-level analysis - more predictors
ap_uv_state <- ap_uv %>%
  select(-c(countyfips, county)) %>%
  group_by(state, year, season) %>%
  #Take medians across county for each season
  summarize(across(pm25_max_pred:i380, median)) %>%
  pivot_wider(names_from = season, values_from = pm25_max_pred:i380)

outcomes_wider_state <- outcomes_state %>%
  pivot_wider(names_from = outcome, values_from = age_adjusted_incidence_rate)
  
# state_analysis <- left_join(outcomes_wider_state, ap_uv_state_slim)
# 
# #Revert EDD variable to be without season
# state_analysis <- state_analysis %>%
#   select(-c())

#Correlations
state_analysis_corr <- state_analysis %>% 
  select(`lung cancer`:edd_Winter) %>% 
  do(as.data.frame(cor(., method="pearson", use="pairwise.complete.obs"))) %>%
  knitr::kable()
state_analysis_corr
```

## Lung Cancer

### Exploration

Before performing our analysis, we may want to look at some bivariate plots with trends over time to see if there are general trends from 2001 to 2015.

```{r, warning=FALSE, message = FALSE}
library(GGally)

state_analysis %>%
  filter(year %in% 2001:2015) %>%
  ggpairs(columns = c("year","lung cancer", "o3_med_pred_Summer", "pm25_med_pred_Summer"),
          mapping = aes(group = state, color = state),
          columnLabels = c("Year", "Lung Cancer/100,000","Median summer O3, ppm", "PM 2.5, ug/m^3"))

  
state_plot <- state_analysis %>%
  ggplot(aes(x = year, y = `lung cancer`, group = state, color = state)) +
  geom_path()

spring_plot <- state_analysis %>%
  ggplot(aes(x = o3_med_pred_Spring, y = `lung cancer`, group = state, color = state)) +
  geom_point()

summer_plot <- state_analysis %>%
  ggplot(aes(x = o3_med_pred_Summer, y = `lung cancer`, group = state, color = state)) +
  geom_point()

fall_plot <- state_analysis %>%
  ggplot(aes(x = o3_med_pred_Fall, y = `lung cancer`, group = state, color = state)) +
  geom_point()

library(patchwork)
(state_plot + spring_plot) / (summer_plot + fall_plot)
```

### Model Fit - Air quality and lung cancer

We construct a model using the air quality variables (ozone, particulate matter), state, and powers and interactions thereof, then pare down with BIC, limiting our focus to the years 2001-2015.

```{r warning=FALSE, message=FALSE}
state_2001_2015 <- state_analysis %>%
  filter(year %in% 2001:2015) %>%
  #exclude asthma, lots of missingness
  select(-asthma)

aq_lung_df <- state_2001_2015 %>%
  select(-c(melanoma, starts_with("edd")))
#fit_aq <- lm(`lung cancer` ~ .^2, data = aq_lung_df) - this broke in R
fit_aq <- lm(`lung cancer` ~ . +
               year*(o3_med_pred_Spring +
                     o3_med_pred_Summer +
                     o3_med_pred_Fall) +
               year*(pm25_med_pred_Spring +
                     pm25_med_pred_Summer +
                     pm25_med_pred_Fall) +
               state*year, data = aq_lung_df)

step_bic_aq <- step(fit_aq, trace = 0, k = log(nobs(fit_aq)), direction = "backward")
summary(step_bic_aq)
```

Am I reading that right? $R^2$ of 0.97! That sounds too good to be true. Let's look at some diagnostic plots.

```{r warning=FALSE, message=FALSE}
lung_fit <- aq_lung_df %>%
  modelr::add_residuals(step_bic_aq) %>%
  modelr::add_predictions(step_bic_aq)

lung_fit %>%
  ggplot(aes(x = pred, y = resid)) + geom_point() + labs(x = "Predicted value", y = "Residual")
```

These residuals look great! Centered around 0 in a relatively even band. How about normality?

```{r warning=FALSE, message=FALSE}
plot(step_bic_aq, which = 2)
```

That's not ideal. I don't love Box-Cox, but I wonder if it might give us some insight.

```{r warning=FALSE, message=FALSE}
MASS::boxcox(step_bic_aq)
```

Really doesn't help us; the best power of our response variable is about 1. Are there any influential points/high-leverage points?

```{r warning=FALSE, message=FALSE}
plot(step_bic_aq, which = 5)
```

No datapoint with a Cook's distance greater than 0.5, that's good. Are we overfit?

### Cross validation

What happens if we cross-validate against simpler models? We will use leave-one-out and Monte Carlo cross-validation.

```{r warning=FALSE, message=FALSE}
library(modelr)
set.seed(15)
cv_df <- crossv_loo(aq_lung_df)
    
cv_df <- cv_df %>% 
  mutate(
    #Ozone only
    ozone  = map(train, ~lm(`lung cancer` ~ o3_med_pred_Spring + o3_med_pred_Summer +
                              o3_med_pred_Fall, data = .x)),
    #Ozone with year and state
    ozone_state_year  = map(train, ~lm(`lung cancer` ~ o3_med_pred_Spring + o3_med_pred_Summer +
                                         o3_med_pred_Fall + year + state, data = .x)),
    #Ozone with year and state with time interactions
    ozone_state_year2  = map(train, ~lm(`lung cancer` ~ year*(o3_med_pred_Spring +
                                                              o3_med_pred_Summer +
                                                              o3_med_pred_Fall) + year + state +
                                                              year*state, data = .x)),
    
    #BIC model
    bic_fit = map(train, ~step_bic_aq, data = .x)) %>% 
  mutate(
    rmse_ozone = map2_dbl(ozone, test, ~rmse(model = .x, data = .y)),
    rmse_ozone_state_year = map2_dbl(ozone_state_year, test, ~rmse(model = .x, data = .y)),
    rmse_ozone_state_year2 = map2_dbl(ozone_state_year2, test, ~rmse(model = .x, data = .y)),
    rmse_bic_fit = map2_dbl(bic_fit, test, ~rmse(model = .x, data = .y)))

cv_df %>% 
  select(starts_with("rmse")) %>% 
  pivot_longer(
    everything(),
    names_to = "model", 
    values_to = "rmse",
    names_prefix = "rmse_") %>% 
  mutate(model = fct_inorder(model)) %>% 
  ggplot(aes(x = model, y = rmse, fill = model)) + geom_violin() + labs(x = "Model", y = "RMSE")
```

Nice! Our BIC fit does the best on leave-one-out. What about Monte Carlo (1000 simulations, holdout 20%)?

```{r warning=FALSE, message=FALSE}
set.seed(15)
cv_df <- crossv_mc(aq_lung_df, n = 1000)
    
cv_df <- cv_df %>% 
  mutate(
    #Ozone only
    ozone  = map(train, ~lm(`lung cancer` ~ o3_med_pred_Spring + o3_med_pred_Summer +
                              o3_med_pred_Fall, data = .x)),
    #Ozone with year and state
    ozone_state_year  = map(train, ~lm(`lung cancer` ~ o3_med_pred_Spring + o3_med_pred_Summer +
                                         o3_med_pred_Fall + year + state, data = .x)),
    #Ozone with year and state with time interactions
    ozone_state_year2  = map(train, ~lm(`lung cancer` ~ year*(o3_med_pred_Spring +
                                                              o3_med_pred_Summer +
                                                              o3_med_pred_Fall) + year + state +
                                                              year*state, data = .x)),
    
    #BIC model
    bic_fit = map(train, ~step_bic_aq, data = .x)) %>% 
  mutate(
    rmse_ozone = map2_dbl(ozone, test, ~rmse(model = .x, data = .y)),
    rmse_ozone_state_year = map2_dbl(ozone_state_year, test, ~rmse(model = .x, data = .y)),
    rmse_ozone_state_year2 = map2_dbl(ozone_state_year2, test, ~rmse(model = .x, data = .y)),
    rmse_bic_fit = map2_dbl(bic_fit, test, ~rmse(model = .x, data = .y)))

cv_df %>% 
  select(starts_with("rmse")) %>% 
  pivot_longer(
    everything(),
    names_to = "model", 
    values_to = "rmse",
    names_prefix = "rmse_") %>% 
  mutate(model = fct_inorder(model)) %>% 
  ggplot(aes(x = model, y = rmse, fill = model)) + geom_violin() + labs(x = "Model", y = "RMSE")
```

Our BIC fit wins again. But there's still something fishy. First, this model has 10 terms and just 45 datapoints - it should almost certainly be overfit, but it still cross-validates well. Second, the $R^2$ for this model is 0.97 - how can that be when it doesn't include smoking, which is probably *the* main determinant of lung cancer? Which variables in this model are the most important?

```{r warning=FALSE, message=FALSE}
broom::tidy(step_bic_aq) %>%
  mutate(
    abs_t = abs(statistic),
    term = as.factor(term),
    term = fct_reorder(term, abs_t)
    ) %>%
  ggplot(aes(x = abs_t, y = term)) +
  geom_col() +
  labs(title = "Relative Importance of Model Terms",
       y = "Model Term", x = "|t| statistic")
```


## Melanoma of the Skin

### Exploration

As before, we create several bivariate plots before constructing our model.

```{r warning=FALSE, message=FALSE}
state_analysis %>%
  filter(year %in% 2001:2015) %>%
  ggpairs(columns = c("year","melanoma","edd_Spring","edd_Summer"),
          mapping = aes(group = state, color = state),
          columnLabels = c("Year", "Melanoma/100,000", "Spring EDD (J/m^2)", "Summer EDD (J/m^2)"))
```

That's weird. There seem to be negative correlations between EDD and melanoma when we break down by state, but positive correlations overall—an instance of Simpson's paradox.

### Model Fit - Melanoma and UV radiation

How well does UV radiation predict melanoma? Let's find out. This time we limit our analysis to 2005-2015.

```{r message=FALSE, warning=FALSE}
uv_df <- state_2001_2015 %>%
  filter(year >= 2005) %>%
  select(-c(starts_with("pm25"), starts_with("o3"), `lung cancer`))
  
mel_fit <- lm(melanoma ~ .^2, data = uv_df)
step_bic_mel <- step(mel_fit, trace = 0, k = log(nobs(mel_fit)), direction = "backward")
summary(step_bic_mel)
```

This model is huge, it's almost certainly overfit. I'm sure the diagnostics will be terrible.

```{r}
mel_df <- uv_df %>%
  modelr::add_residuals(step_bic_mel) %>%
  modelr::add_predictions(step_bic_mel)

mel_df %>%
  ggplot(aes(x = pred, y = resid)) + geom_point() + labs(x = "Predicted value", y = "Residual")
```

Residuals suggest a bit of heteroscedasticity and curvature but not as bad as I imagined. How about normality?

```{r warning=FALSE, message=FALSE}
plot(step_bic_mel, which = 2)
```

Definitely not normal. High leverage points?

```{r warning=FALSE, message=FALSE}
plot(step_bic_mel, which = 5)
```

Several high leverage points. I think we want a different model. Lets' try a simple one without interactions.

```{r message=FALSE, warning=FALSE}
mel_fit2 <- lm(melanoma ~ ., data = uv_df)
summary(mel_fit2)
```

This comes at a big cost to explanatory power. Notably, the EDD variables aren't significant. How do the residuals look?

```{r message=FALSE, warning=FALSE}
mel_df2 <- uv_df %>%
  modelr::add_residuals(mel_fit2) %>%
  modelr::add_predictions(mel_fit2)

mel_df2 %>%
  ggplot(aes(x = pred, y = resid)) + geom_point() + labs(x = "Predicted value", y = "Residual")
```

We have curvature in the residuals. What is the source?

```{r message=FALSE, warning=FALSE}
summer_resid <- mel_df2 %>%
  ggplot(aes(x = edd_Summer, y = resid)) + geom_point() + labs(x = "Summer Median EDD", y = "Residual")

year_resid <- mel_df2 %>%
  ggplot(aes(x = year, y = resid)) + geom_point() + labs(x = "Year", y = "Residual")

fall_resid <- mel_df2 %>%
  ggplot(aes(x = edd_Fall, y = resid)) + geom_point() + labs(x = "Fall Median EDD", y = "Residual")

state_resid <- mel_df2 %>%
  ggplot(aes(x = state, y = resid)) + geom_violin() + labs(x = "State", y = "Residual")

(summer_resid + year_resid) / (fall_resid + state_resid)
```

Seems like fall EDD and year might be the issues. Try transforming.

```{r}
mel_fit3 <- lm(melanoma ~ state + I(1/year^3) + 
                 edd_Spring + edd_Summer + edd_Fall + edd_Winter,
                 data = uv_df)
summary(mel_fit3)

mel_df3 <- uv_df %>%
  modelr::add_residuals(mel_fit3) %>%
  modelr::add_predictions(mel_fit3)

mel_df3 %>%
  ggplot(aes(x = pred, y = resid)) + geom_point() + labs(x = "Predicted value", y = "Residual")
```

Not much better. How are the other diagnostics?

```{r}
plot(mel_fit2, which = 2)
```

Not very normal...

```{r}
plot(mel_fit2, which = 5)
```

No high-leverage points. Are we overfit? We only have 33 rows.

### Cross validation

```{r message=FALSE, warning=FALSE}
set.seed(15)
cv_df <- crossv_loo(uv_df)
    
cv_df <- cv_df %>% 
  mutate(
    #State and year only
    state_year = map(train, ~lm(melanoma ~ state + year, data = .x)),
    
    #EDD only
    edd = map(train, ~lm(melanoma ~ edd_Winter + edd_Spring + edd_Summer + edd_Fall, data = .x)),
    
    #EDD with year and state, linear
    edd_state_year = map(train, ~mel_fit2, data = .x),
    
    #EDD with year and state with time interactions
    edd_state_year2  = map(train, ~lm(melanoma ~ year*(edd_Winter +
                                                       edd_Spring +
                                                       edd_Summer +
                                                       edd_Fall) + year + state +
                                                       year*state, data = .x)),
    
    #BIC model
    bic_fit = map(train, ~step_bic_mel, data = .x)) %>% 
  mutate(
    rmse_state_year = map2_dbl(state_year, test, ~rmse(model = .x, data = .y)),
    rmse_edd = map2_dbl(edd, test, ~rmse(model = .x, data = .y)),
    rmse_edd_state_year = map2_dbl(edd_state_year, test, ~rmse(model = .x, data = .y)),
    rmse_edd_state_year2 = map2_dbl(edd_state_year2, test, ~rmse(model = .x, data = .y)),
    rmse_bic_fit = map2_dbl(bic_fit, test, ~rmse(model = .x, data = .y)))

cv_df %>% 
  select(starts_with("rmse")) %>% 
  pivot_longer(
    everything(),
    names_to = "model", 
    values_to = "rmse",
    names_prefix = "rmse_") %>% 
  mutate(model = fct_inorder(model)) %>% 
  ggplot(aes(x = model, y = rmse, fill = model)) + geom_violin() + labs(x = "Model", y = "RMSE")
```

The BIC fit wins? How? Will it survive 1000 Monte Carlo simulations?

```{r message=FALSE, warning=FALSE}
set.seed(15)
cv_df <- crossv_mc(uv_df, 1000)

cv_df <- cv_df %>% 
  mutate(
    #State and year only
    state_year = map(train, ~lm(melanoma ~ state + year, data = .x)),
    
    #EDD only
    edd = map(train, ~lm(melanoma ~ edd_Winter + edd_Spring + edd_Summer + edd_Fall, data = .x)),
    
    #EDD with year and state, linear
    edd_state_year = map(train, ~mel_fit2, data = .x),
    
    #EDD with year and state with time interactions
    edd_state_year2  = map(train, ~lm(melanoma ~ year*(edd_Winter +
                                                       edd_Spring +
                                                       edd_Summer +
                                                       edd_Fall) + year + state +
                                                       year*state, data = .x)),
    
    #BIC model
    bic_fit = map(train, ~step_bic_mel, data = .x)) %>% 
  mutate(
    rmse_state_year = map2_dbl(state_year, test, ~rmse(model = .x, data = .y)),
    rmse_edd = map2_dbl(edd, test, ~rmse(model = .x, data = .y)),
    rmse_edd_state_year = map2_dbl(edd_state_year, test, ~rmse(model = .x, data = .y)),
    rmse_edd_state_year2 = map2_dbl(edd_state_year2, test, ~rmse(model = .x, data = .y)),
    rmse_bic_fit = map2_dbl(bic_fit, test, ~rmse(model = .x, data = .y)))

cv_df %>% 
  select(starts_with("rmse")) %>% 
  pivot_longer(
    everything(),
    names_to = "model", 
    values_to = "rmse",
    names_prefix = "rmse_") %>% 
  mutate(model = fct_inorder(model)) %>% 
  ggplot(aes(x = model, y = rmse, fill = model)) + geom_violin() + labs(x = "Model", y = "RMSE")
```

That model *must* be disgustingly overfit... right? How important is each variable?

```{r warning=FALSE, message=FALSE}
broom::tidy(step_bic_mel) %>%
  mutate(
    abs_t = abs(statistic),
    term = as.factor(term),
    term = fct_reorder(term, abs_t)
    ) %>%
  ggplot(aes(x = abs_t, y = term)) +
  geom_col() +
  labs(title = "Relative Importance of Model Terms",
       y = "Model Term", x = "|t| statistic")
```

However, we see a different pattern of importance from the model with just linear terms:

```{r warning=FALSE, message=FALSE}
broom::tidy(mel_fit2) %>%
  mutate(
    abs_t = abs(statistic),
    term = as.factor(term),
    term = fct_reorder(term, abs_t)
    ) %>%
  ggplot(aes(x = abs_t, y = term)) +
  geom_col() +
  labs(title = "Relative Importance of Model Terms",
       y = "Model Term", x = "|t| statistic")
```

# Part 2: Cross-Sectional County-Level Trends, 2014-2018

Time is a significant predictor in many of the above models. Will we still have good models without it? In exchange, we get a lot more data - resolution at the county level instead of the state level.

We create our county-level dataframe:

```{r warning=FALSE, message=FALSE}
outcomes_wider_county <- outcomes_county %>%
  pivot_wider(names_from = outcome, values_from = age_adjusted_incidence_rate)

#county-level analysis - fewer predictors
ap_uv_county_slim <- ap_uv %>%
  filter(state %in% c("NY","OH","PA")) %>%
  select(-c(pm25_max_pred, pm25_mean_pred, pm25_pop_pred,
            o3_max_pred, o3_mean_pred, o3_pop_pred, i324,
            i310, i305, i380, edr)) %>%
  group_by(county, state, season) %>%
  #Take medians across years for each season
  summarize(across(pm25_med_pred:edd, median, na.rm = TRUE)) %>%
  pivot_wider(names_from = season, values_from = pm25_med_pred:edd)

county_df <- right_join(outcomes_wider_county, ap_uv_county_slim)
```

Again, we look at correlations among the predictors:

```{r warning=FALSE, message=FALSE}
county_corr <- county_df %>%
  select(pm25_med_pred_Fall:edd_Winter) %>% 
  do(as.data.frame(cor(., method="pearson", use="pairwise.complete.obs"))) %>%
  knitr::kable()
county_corr
```

We average the particulate matter because the seasonal variables are highly correlated.

```{r message=FALSE, warning=FALSE}
county_df2 <- county_df %>%
  mutate(pm25_med_pred = rowMeans(select(., starts_with("pm25")))) %>%
  select(-c(pm25_med_pred_Fall:pm25_med_pred_Winter))
```

Now we can start the cross-sectional analysis.

## Lung cancer, cross-sectional analysis

### Exploration

```{r message=FALSE, warning=FALSE}
county_df2 %>%
  ggpairs(columns = c("lung cancer", "o3_med_pred_Summer", "o3_med_pred_Fall", "pm25_med_pred"),
          mapping = aes(group = state, color = state),
          columnLabels = c("Lung Cancer/100,000","Median summer O3, ppm", "Median fall O3, ppm",
                           "PM 2.5, ug/m^3"))
```

Some of these correlations seem quite flat, while others rather strong, and these trends can reverse when we break down by state.

### Model fit - lung cancer and air quality, cross-sectional data

Time for another model. Once more we try to predict lung cancer as a function of air quality variables.

```{r message=FALSE, warnin=FALSE}
aq_lung_df2 <- county_df2 %>%
  select(-c(melanoma, asthma, starts_with("edd"), county, fips))

fit_aq_cty <- lm(`lung cancer` ~ .^2, data = aq_lung_df2)

step_bic_aq_cty <- step(fit_aq_cty, trace = 0, k = log(nobs(fit_aq)), direction = "backward")
summary(step_bic_aq_cty)
```

$R^2 = 0.37$ is much more reasonable than what we were seeing before. Interestingly this model doesn't have any particulate matter terms. How do the diagnostics look?

```{r warning=FALSE, message=FALSE}
lung_fit_cty <- aq_lung_df2 %>%
  modelr::add_residuals(step_bic_aq_cty) %>%
  modelr::add_predictions(step_bic_aq_cty)

lung_fit_cty %>%
  ggplot(aes(x = pred, y = resid)) + geom_point() + labs(x = "Predicted value", y = "Residual")
```

We have some heteroscedasticity, but otherwise beautiful residuals. How about normality?

```{r message=FALSE, warning=FALSE}
plot(step_bic_aq_cty, which = 2)
```

Not perfect, but a lot better than the state-level model. High-leverage points?

```{r message=FALSE, warning=FALSE}
plot(step_bic_aq_cty, which = 5)
```

No high-leverage points.

### Cross-validation

Are we overfit? We have 217 rows. Use Monte Carlo cross-validation against simpler models.

```{r message=FALSE, warning=FALSE}
set.seed(15)
cv_df <- crossv_mc(aq_lung_df2, 1000)

cv_df <- cv_df %>% 
  mutate(
    #PM only
    pm = map(train, ~lm(`lung cancer` ~ pm25_med_pred, data = .x)),
    
    #O3 only
    o3 = map(train, ~lm(`lung cancer` ~ o3_med_pred_Fall +
                           o3_med_pred_Winter +
                           o3_med_pred_Spring +
                           o3_med_pred_Summer, data = .x)),
    
    #O3 and PM
    o3_pm = map(train, ~lm(`lung cancer` ~ o3_med_pred_Fall +
                           o3_med_pred_Winter +
                           o3_med_pred_Spring +
                           o3_med_pred_Summer +
                           pm25_med_pred, data = .x)),
    
    #O3, PM, and State
    o3_pm_state  = map(train, ~lm(`lung cancer` ~ ., data = .x)),
    
    #BIC model
    bic_fit = map(train, ~step_bic_aq_cty, data = .x)) %>% 
  mutate(
    rmse_pm25 = map2_dbl(pm, test, ~rmse(model = .x, data = .y)),
    rmse_o3 = map2_dbl(o3, test, ~rmse(model = .x, data = .y)),
    rmse_o3_pm25 = map2_dbl(o3_pm, test, ~rmse(model = .x, data = .y)),
    rmse_o3_pm25_state = map2_dbl(o3_pm_state, test, ~rmse(model = .x, data = .y)),
    rmse_bic_fit = map2_dbl(bic_fit, test, ~rmse(model = .x, data = .y)))

cv_df %>% 
  select(starts_with("rmse")) %>% 
  pivot_longer(
    everything(),
    names_to = "model", 
    values_to = "rmse",
    names_prefix = "rmse_") %>% 
  mutate(model = fct_inorder(model)) %>% 
  ggplot(aes(x = model, y = rmse, fill = model)) + geom_violin() + labs(x = "Model", y = "RMSE")
```

Once again the BIC model wins. But we actually shouldn't be upset about it this time. Our model has 11 terms and 217 rows—it's not like we only have 45 rows for that many predictors. Maybe it really isn't overfit! How important are the predictors relative to one another?

```{r warning=FALSE, message=FALSE}
broom::tidy(step_bic_aq_cty) %>%
  mutate(
    abs_t = abs(statistic),
    term = as.factor(term),
    term = fct_reorder(term, abs_t)
    ) %>%
  ggplot(aes(x = abs_t, y = term)) +
  geom_col() +
  labs(title = "Relative Importance of Model Terms",
       y = "Model Term", x = "|t| statistic")
```

The strongest terms in this model are the state-by-ozone interactions. I wonder how much policy and industry differ between the two states.

## Melanoma of the skin, cross-sectional analysis

```{r warning=FALSe, message=FALSE}
county_df2 %>%
  ggpairs(columns = c("melanoma", "edd_Summer", "edd_Fall", "edd_Winter", "edd_Spring"),
          mapping = aes(group = state, color = state),
          columnLabels = c("Melanoma/100,000","Median summer EDD", "Median fall EDD",
                           "Median winter EDD", "Median spring EDD"))
```

No season has a strong correlation with melanoma, but the seasons are moderately to strongly correlated with one another, spring and fall especially.

### Model fit, melanoma and solar UV radiation, cross-sectional data

```{r message=FALSE, warning=FALSE}
mel_cty <- county_df2 %>%
  select(state, melanoma, starts_with("edd"))

fit_mel_cty <- lm(melanoma ~ .^2 +
                    I(edd_Spring^3) +
                    I(edd_Summer^3) +
                    I(edd_Fall^3) +
                    I(edd_Winter^3), data = mel_cty)

step_bic_mel_cty <- step(fit_mel_cty, trace = 0, k = log(nobs(fit_aq)), direction = "backward")
summary(step_bic_mel_cty)
```

Once again we have a relatively modest $R^2$ (0.23). How do the residuals look?

```{r warning=FALSE, message=FALSE}
mel_fit_cty <- mel_cty %>%
  modelr::add_residuals(step_bic_mel_cty) %>%
  modelr::add_predictions(step_bic_mel_cty)

mel_fit_cty %>%
  ggplot(aes(x = pred, y = resid)) + geom_point() + labs(x = "Predicted value", y = "Residual")
```

We have some heteroscedasticity but doesn't look all that bad. Is it normal?

```{r message=FALSE, warning=FALSE}
plot(step_bic_mel_cty, which = 2)
```

Not ideal, but mostly ok. High-leverage points?

```{r message=FALSE, warning=FALSE}
plot(step_bic_mel_cty, which = 5)
```

No high-leverage points (Cook's D >0.5).

### Cross validation

```{r message=FALSE, warning=FALSE}
set.seed(15)
cv_df <- crossv_mc(mel_cty, 1000)

cv_df <- cv_df %>% 
  mutate(
    #Summer EDD only
    summer_edd = map(train, ~lm(melanoma ~ edd_Summer, data = .x)),
    
    #Summer and fall
    summer_fall = map(train, ~lm(melanoma ~ edd_Summer + edd_Fall, data = .x)),
    
    #Four seasons
    four_seasons = map(train, ~lm(melanoma ~ edd_Fall +
                           edd_Winter +
                           edd_Spring +
                           edd_Summer, data = .x)),
    
    #Four seasons and state
    seasons_state  = map(train, ~lm(melanoma ~ ., data = .x)),
    
    #BIC model
    bic_fit = map(train, ~step_bic_mel_cty, data = .x)) %>% 
  mutate(
    rmse_summer_edd = map2_dbl(summer_edd, test, ~rmse(model = .x, data = .y)),
    rmse_summer_fall = map2_dbl(summer_fall, test, ~rmse(model = .x, data = .y)),
    rmse_four_seasons = map2_dbl(four_seasons, test, ~rmse(model = .x, data = .y)),
    rmse_seasons_state = map2_dbl(seasons_state, test, ~rmse(model = .x, data = .y)),
    rmse_bic_fit = map2_dbl(bic_fit, test, ~rmse(model = .x, data = .y)))

cv_df %>% 
  select(starts_with("rmse")) %>% 
  pivot_longer(
    everything(),
    names_to = "model", 
    values_to = "rmse",
    names_prefix = "rmse_") %>% 
  mutate(model = fct_inorder(model)) %>% 
  ggplot(aes(x = model, y = rmse, fill = model)) + geom_violin() + labs(x = "Model", y = "RMSE")
```

It looks like the interaction terms matter. Just how important are they?

```{r warning=FALSE, message=FALSE}
broom::tidy(step_bic_mel_cty) %>%
  mutate(
    abs_t = abs(statistic),
    term = as.factor(term),
    term = fct_reorder(term, abs_t)
    ) %>%
  ggplot(aes(x = abs_t, y = term)) +
  geom_col() +
  labs(title = "Relative Importance of Model Terms",
       y = "Model Term", x = "|t| statistic")
```

That's unintuitive. One might expect summer EDD would have the biggest effect on melanoma, but in this model the winter and spring EDD terms are the most important.
